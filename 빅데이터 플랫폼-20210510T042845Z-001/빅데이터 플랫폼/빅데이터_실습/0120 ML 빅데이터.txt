
데이터 파이프라인 시스템 구축
- 빅데이터 기반이기 때문에 빅데이터 업무에 대한 부분을 설명

- 수집 -> 처리 -> 저장 -> 분석 -> 가시화

 Beats - ELK의 경량 수집기, filebeat, metricbeat, packetbeat, functionbeat, auditbeat, winlogbeat
  => 수집시에 데이터 전송 형태/목적지/프로토콜

 logstash - 데이터 처리 파이프라인, input, filter, output
  input : 어떤 데이터를 입력 가능한지, beats, file, http 등등
  filter : 필터링/변환(transform)
  output : elasticsearch, db,

 elasticsearch : 아파치 루씬 검색 기반, 검색/저장, RESTful 시스템/REST api를 사용해서 데이터 처리
  json, == index

  kibana = 가시화, grafana

 data warehouse
 data lake

 데이터를 정형화하는 방법

 빅데이터
 1. 하둡 에코시스템
  - https://projects.apache.org/projects.html
  - 라이선스 : 아파치, MIT, GPL, BSD, 
  - hadoop, hdfs(xfs, ext4), zookeeper, kafka(위치가 중요), (parquet), spark, elasticsearch(lucene, solr)
    sqoop/flume, Ignite, NiFi, .....

  - 데이터 전처리 과정 
    : logstash

 가상화 
  - 가상 머신 : virtualbox, VMWare
  - 컨테이너 : docker ( Microservice, devops)

 kubernetis = k8s

 2. 클라우드
 - public cloud
 - private cloud
 - Iaas(Infrastructure), Paas(Platform), Saas(Software), Faas(Function)
 - AWS

 openstack
 cloudstack
 
 가상화 (vmware, docker(container) - k8s

 프로그램/데이터분석/데이터 파이프라인 구축  

3. 블록체인
 - 프라이빗 블록체인
 - 퍼블릭 블록체인
 - 하이브리드 블록체인

4. 인공지능 
 - 머신러닝
 - 딥러닝

 실시간 지능형 시스템
 
big data
small data

시계열 데이터
IoT 센서 데이트를 어떤 포맷과 프로토콜로 데이터를 전송받을건지 조사
 - json
 - http, MQTT(publish/subscribe)

logstash - csv => filter .conf 형식/문법

RPA
spark/
bin
data
 실습
1. ELK + Beats 구축 : /var/log/messages 파일 실시간 수집 스트리밍
 /etc/logstash/pipelines.yml
  -  pipeline.id : main
     path.config : /etc/logstash/conf.d/log_data.conf (기존 *.conf를 변경)
 -  pipeline.id : bitcoin
     path.config : /etc/logstash/conf.d/bitcoint.conf    

2. csv 파일을 구해서(케글, data.go.kr 참고) 적재
  https://bkjeon1614.tistory.com/312  (인구분석)
  https://jtoday.tistory.com/56 (데이터 소스 위치를 확인)
 - ES에 curl -XPUT에 적재 가능
 - logstash를 통해서 ES 적재, pipelines.yml을 수정해서 2개의 파이프라인으로 처리 - Kibana
2-1. Open API 기반 ELK 구축
 - 참고 : https://kitwaicloud.github.io/elk/weather.html
 - logstash를 통해서 데이터 수집 및 ES 적재, 3번째 pipeline을 통해서 ES에 적재
 
3. logstash(input, filter, output)를 이용해서 데이터 필터링/변환해서 ES 적재
4. Spark를 설치 및 운용(스칼라/파이썬 가능)해서 ES에서 데이터 로딩/수정-ES적재
5. Hadoop
6. kafka
splunk
jps
jdk 설치 ==> jdk와 jre
jre
which java
ls -l /bin/java
cd /etc/alternatives/
ls -l java
yum install java-1.8.0-openjdk
yum install java-1.8.0-openjdk-devel

geoip

vi /etc/filebeat/filebeat.yml

:28
 #- /var/log/m
   - /var/log/messages

  output
  elasticsearch

  logstash

  kafka

 /etc/services

alternatives --config java

cd /etc/logstash
vi logstash.yml
/worker
/automatic

CEP (complex event processing)  - esper

vi /etc/elasticsearch/elasticsearch.yml

vi /var/log/messages

/var/log/logstash
/var/log/elasticsearch

tail -f /var/log/messages

grep -c processor /proc/cpuinfo
grep ^processor /proc/cpuinfo | wc -l

cd /var/lib/elasticsearch/nodes/0/indices state
ls -a

반장님
옆반은 별도로 프로젝트로 하지 않았어요
그리고 제안한 비지도 학습 기반 시계열 데이터 이상탐지 프로젝트도 하지 않았구요
시간 제약때문에 ELK와 빅데이터 하둡 기반 데이터 처리 부분만 하고 마무리했어요
SD반은 제가 미니프로젝트를 하기 때문에 그떄 하기로 했어요

하지만 ML반은 시간이 있어서 나중 과정을 위해서도 3일 정도 텀 프로젝트를 하고자 합니다.
기간이 짧고 완성도도 떨어지겠지만 시도하고 조사하고 구축하면서 이후 과정에 도움이 되고자 하는 의도입니다.
팀별로 데이터와 목표는 동일하게 했으면 합니다.

구체적인 데이터는 이후 말씀드릴 예정이구요
현재는 조사가 필요합니다.
조사는 각 팀별로 역할을 정해서 하시면 좋을듯해요

spark.apache.org
tar xvzf spark-2.4.7-bin-hadoop2.7.tgz -C /home
ln -s spark-2.4.7-bin-hadoop2.7 spark

.bash_profile
SPARK_HOME=/home/spark

export PATH=.....:$SPARK_HOME/bin

source /root/.bash_profile

flume flume-ng

top

nice
renice

vi /var/log/messages
<info>
[INFO]

자동으로 학습하고 모델 만들고 모델 배포해서 이상 탐지 자동화 하자
추천도 개인 데이터/성향...받아서 
















